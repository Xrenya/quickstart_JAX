{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade jax jaxlib"
      ],
      "metadata": {
        "id": "zgQ1_rjQ5UV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj12gmPfS_Na"
      },
      "outputs": [],
      "source": [
        "import jax  # NOTE: jax-0.7.2, jaxlib-0.7.2 the lib is constantl developing so it might be outdated\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "from jax import random\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from typing import Tuple, NamedTuple\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = 0.\n",
        "\n",
        "def impure_uses_global(x):\n",
        "    return x + g\n",
        "\n",
        "print(f\"First call: {jit(impure_uses_global)(4.)}\")\n",
        "g = 10.  # update global state\n",
        "\n",
        "\n",
        "print(f\"Second call: {jit(impure_uses_global)(5.)}\")  # used cached value"
      ],
      "metadata": {
        "id": "LMqqZ3IbmwvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "state = jax.random.PRNGKey(seed)\n",
        "\n",
        "state1, state2 = jax.random.split(state)"
      ],
      "metadata": {
        "id": "j2kQOqPLnEUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Counter:\n",
        "    def __init__(self):\n",
        "        self.n = 0\n",
        "\n",
        "    def count(self) -> int:\n",
        "        self.n += 1\n",
        "        return self.n\n",
        "\n",
        "    def reset(self):\n",
        "        self.n = 0\n",
        "\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "for _ in range(3):\n",
        "    print(counter.count())"
      ],
      "metadata": {
        "id": "S-DudNjfoJh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter.reset()\n",
        "fast_count = jax.jit(counter.count)\n",
        "\n",
        "for _ in range(3):  # oops, it's not working as it's supposed to be\n",
        "    print(fast_count())"
      ],
      "metadata": {
        "id": "b_hms6ejoLMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import make_jaxpr\n",
        "\n",
        "\n",
        "counter.reset()\n",
        "print(make_jaxpr(counter.count)())"
      ],
      "metadata": {
        "id": "9UnMcewNYZC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CounterState = int\n",
        "\n",
        "class CounterV2:\n",
        "    def count(self, n: CounterState) -> Tuple[int, CounterState]:\n",
        "        return n + 1, n + 1\n",
        "\n",
        "    def reset(self) -> CounterState:\n",
        "        return 0\n",
        "\n",
        "\n",
        "counter = CounterV2()\n",
        "state = counter.reset()\n",
        "\n",
        "for _ in range(3):\n",
        "    value, state = counter.count(state)\n",
        "    print(value)"
      ],
      "metadata": {
        "id": "NumtS-z0Yl3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = counter.reset()\n",
        "fast_count = jax.jit(counter.count)\n",
        "\n",
        "for _ in range(3):\n",
        "    value, state = counter.count(state)\n",
        "    print(value)"
      ],
      "metadata": {
        "id": "1wNxXyuAZMKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary we used the following rule to convert a stateful class:\n",
        "```python\n",
        "class StatefulClass\n",
        "\n",
        "    state: State\n",
        "\n",
        "    def stateful_method(*args, **kwargs) -> Output:\n",
        "        NotImplemented\n",
        "\n",
        "```\n",
        "into a class of the form:\n",
        "```python\n",
        "class StatelessClass\n",
        "\n",
        "    def stateless_method(state: State, *args, **kwargs) -> (Output, State):\n",
        "        NotImplemented\n",
        "\n",
        "```\n",
        "Nice - we figured an equivalent way to handle states without introducing the side-effects.\n",
        "\n",
        "This brings us 1 step closer to building neural networks! 🥳\n",
        "\n",
        "We still need to find a way to handle gradients when dealing with big NNs.\n",
        "\n",
        "In nutshell, jax requires random_key/state everywhere"
      ],
      "metadata": {
        "id": "DDhA-QSRZx9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter PyTree 🌳\n",
        "Before we start - why are gradients a problem in the first place?"
      ],
      "metadata": {
        "id": "Cp9H6y5lbARV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTree basics"
      ],
      "metadata": {
        "id": "_tY9BbZtbK80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x, y, z, w: x**2 + y**2 + z**2 + w**2\n",
        "\n",
        "# JAX: .backward() is not that great\n",
        "# also JAX:\n",
        "x, y, z, w = [1. for _ in range(4)]\n",
        "dfdx, dfdy, dfdz, dfdw = grad(f, argnums=(0, 1, 2, 3))(x, y, z, w)\n",
        "print(dfdx, dfdy, dfdz, dfdw)\n",
        "\n",
        "# Great now we just need to update our params!\n",
        "# lr = 0.001\n",
        "# x -= lr*dfdx\n",
        "# y -= lr*dfdy\n",
        "# ... (175B lines later)\n",
        "# w -= lr*dfdw\n",
        "\n",
        "# No, no, no.\n",
        "# We do have a better way."
      ],
      "metadata": {
        "id": "oHuBvb_2Zfjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, no, no. We do have a better way!\n",
        "\n",
        "We want to, more naturally, wrap our params in some more complex data structures like dictionaries, etc.\n",
        "\n",
        "JAX knows how to deal with these! The answer is called a PyTree."
      ],
      "metadata": {
        "id": "87tmS8m2c5XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A contrived example for pedagogical purposes\n",
        "# (if your mind needs to attach some semantics to parse this - treat it as model params)\n",
        "\n",
        "pytree_example = [\n",
        "    [1, 'a', object()],\n",
        "    (1, (2, 3), ()),\n",
        "    [1, {'k': 2, 'k2': (3, 4)}, 5],\n",
        "    {'a': 2, 'b': (2, 3)},\n",
        "    jnp.array([1, 2, 3]),\n",
        "    jnp.array([[1, 2, 3]]),\n",
        "]\n",
        "\n",
        "for pytree in pytree_example:\n",
        "    leaves = jax.tree.leaves(pytree)\n",
        "    print(f\"{repr(pytree):<45} has {len(leaves)} leaves: {leaves}\")"
      ],
      "metadata": {
        "id": "B0iTuWnPb3hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we manipulate PyTrees?\n",
        "\n",
        "list_of_list = [\n",
        "    {'a': 3},\n",
        "    [1, 2, 3],\n",
        "    [1, 2],\n",
        "    [1, 2, 3, 4]\n",
        "]\n",
        "\n",
        "# For single arg functions use tree_map\n",
        "# tree_map iterates through leaves and applies the lambda function\n",
        "# NOTE jax.tree_util.tree_map is an alias of jax.tree.map().\n",
        "print(jax.tree_util.tree_map(lambda x: x*2, list_of_list))"
      ],
      "metadata": {
        "id": "dPwdb-Q3e_H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "another_list_of_lists = list_of_list\n",
        "# jax.tree_util.tree_multimap, replace it with jax.tree.map as they are functionally identical in this context.\n",
        "print(jax.tree.map(lambda x, y: x + y, list_of_list, another_list_of_lists))"
      ],
      "metadata": {
        "id": "Q73wbKvC2d68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTrees need to have the same structure if we are to apply tree.map()\n",
        "another_list_of_lists = deepcopy(list_of_list)\n",
        "another_list_of_lists.append([23])  # not the same structure anymore\n",
        "print(jax.tree.map(lambda x, y: x + y, list_of_list, another_list_of_lists))  # ValueError"
      ],
      "metadata": {
        "id": "N8vxJT-z5Z5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Less contrived example: training a toy MLP (multi-layer perceptron) model"
      ],
      "metadata": {
        "id": "s_lCWcGw81eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_mlp_params(layer_width):\n",
        "    params = []\n",
        "\n",
        "    # Allocate weights and biases (model parameters)\n",
        "    # Notice: we're not using JAX's PRNG here - doesn't matter for this simple example\n",
        "    for n_in, n_out in zip(layer_width[:-1], layer_width[1:]):\n",
        "        params.append(\n",
        "            dict(\n",
        "                weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2. / n_in),\n",
        "                bias=np.ones(shape=(n_out,))\n",
        "            )\n",
        "        )\n",
        "    return params\n",
        "\n",
        "# Instantiate a single input - single output, 3 layer (2 hidden layers) deep MLP\n",
        "params = init_mlp_params([1, 128, 128, 1])\n",
        "\n",
        "# Another example of how we might use tree_map - verify that shapes make sense:\n",
        "jax.tree.map(lambda x: x.shape, params)"
      ],
      "metadata": {
        "id": "ecnRCFjR8sAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(params, x):\n",
        "    *hidden, last = params\n",
        "\n",
        "    for layer in hidden:\n",
        "        x = jax.nn.relu(jnp.dot(x, layer[\"weights\"]) + layer[\"bias\"])\n",
        "\n",
        "    return jnp.dot(x, last[\"weights\"]) + last[\"bias\"]\n",
        "\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    return jnp.mean((forward(params, x) - y) ** 2)  # MSE Loss\n",
        "\n",
        "\n",
        "lr = 0.0001\n",
        "\n",
        "@jit # notice how we do jit only at the highest level - XLA will have plenty of space to optimize\n",
        "def update(params, x, y):\n",
        "\n",
        "    # Note that grads is a pytree with the same structure as params.\n",
        "    # grad is one of the many JAX functions that has built-in support for pytrees!\n",
        "    grads = jax.grad(loss_fn)(params, x, y)\n",
        "\n",
        "    # Task: analyze grads and make sure it has the same structure as param\n",
        "\n",
        "    # SGD update\n",
        "    return jax.tree.map(lambda p, g: p - lr * g, params, grads)"
      ],
      "metadata": {
        "id": "cYIABkzh9eRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = np.random.normal(size=(128, 1))\n",
        "ys = xs**2  # let's learn how to regress a parabola\n",
        "\n",
        "# Task experiment a bit with other functions (polynomials, sin, etc.)\n",
        "\n",
        "num_epochs = 5000\n",
        "for _ in range(num_epochs):\n",
        "    params = update(params, xs, ys)  # again our lovely pattern\n",
        "\n",
        "plt.scatter(xs, ys, label=\"GT\", c=\"g\", marker=\".\")\n",
        "plt.scatter(xs, forward(params, xs), label=\"Model prediction\", c=\"r\", marker=\".\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5v8cFkoG-efu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wohoo! We've trained our first neural network! 🥳🥳🥳\n",
        "\n",
        "In order to be able to build NN libs and layers such as nn.Linear (PyTorch syntax), etc. we need a couple more tricks up our sleeves! 🔥"
      ],
      "metadata": {
        "id": "RRtIb_w9_D-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom PyTrees ⚙️"
      ],
      "metadata": {
        "id": "CH0m38ul_WKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyContainer:  # this could be a linear layer a conv layer or whatever\n",
        "    def __init__(self, name: str, a: int, b: int, c: int):\n",
        "        self.name = name\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.c = c"
      ],
      "metadata": {
        "id": "5ENU0gPZ-_yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_pytree = [MyContainer(\"Alice\", 1, 2, 3), MyContainer(\"Bob\", 1, 2, 3)]  # 8 leaves? Right? Noup.\n",
        "\n",
        "leaves = jax.tree.leaves(example_pytree)\n",
        "print(f\"{repr(example_pytree):<45}\\n has {len(leaves)} leaves:\\n {leaves}\")"
      ],
      "metadata": {
        "id": "WqHQQeTvKOPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.tree.map(lambda x: x + 1, example_pytree))  # this will not work :/ it'd be nice if it did"
      ],
      "metadata": {
        "id": "SVgMH0cuKfl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get it to work! We'll need to define 2 functions (flatten/unflatten)\n",
        "\n",
        "def flatten_MyContainer(container):\n",
        "    \"\"\"Returns an iterable over container contents, and aux data.\"\"\"\n",
        "    flatten_contents = [container.a, container.b, container.c]\n",
        "\n",
        "    # we don't want the name to appear as a child, so it is auxiliary data.\n",
        "    # auxiliary data is usually a description of the structure of a node,\n",
        "    # e.g., the keys of a dict -- anything that isn't a node's children.\n",
        "    aux_data = container.name\n",
        "    return flatten_contents, aux_data\n",
        "\n",
        "\n",
        "def unflatten_MyContainer(aux_data, flat_contents):\n",
        "    \"\"\"Converts aux data and the flat contents into a MyContainer.\"\"\"\n",
        "    return MyContainer(aux_data, *flat_contents)\n",
        "\n",
        "# Register a custom PyTree node\n",
        "jax.tree_util.register_pytree_node(MyContainer, flatten_MyContainer, unflatten_MyContainer)"
      ],
      "metadata": {
        "id": "QHpQGasDKqr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try again!\n",
        "leaves = jax.tree.leaves(example_pytree)\n",
        "print(f\"{repr(example_pytree):<45}\\n has {len(leaves)} leaves:\\n {leaves}\")"
      ],
      "metadata": {
        "id": "LLzYXlyVNHo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = jax.tree.map(lambda x: x + 1, example_pytree)\n",
        "print(jax.tree.leaves(result))  # it works now as expected!"
      ],
      "metadata": {
        "id": "NRJGQUPGNQMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally a common gotcha working with PyTrees: mistaking nodes for leaves/children\n",
        "\n",
        "zeros_tree = [jnp.zeros((2, 3)), jnp.zeros((3, 4))]\n",
        "print(zeros_tree)\n",
        "\n",
        "# Try to make another tree with ones instead of zeros\n",
        "shapes = jax.tree.map(lambda x: x.shape, zeros_tree)\n",
        "print(shapes)\n",
        "\n",
        "ones_tree = jax.tree.map(jnp.ones, shapes)\n",
        "print(ones_tree)"
      ],
      "metadata": {
        "id": "BRz1O-a_Np6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally a common gotcha working with PyTrees: mistaking nodes for leaves/children\n",
        "\n",
        "zeros_tree = [jnp.zeros((2, 3)), jnp.zeros((3, 4))]\n",
        "print(zeros_tree)\n",
        "\n",
        "# Try to make another tree with ones instead of zeros\n",
        "ones_tree = jax.tree.map(lambda x: x + 1, zeros_tree)\n",
        "print(ones_tree)"
      ],
      "metadata": {
        "id": "f9b_WvX9OixI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We can now create custom layers and we can train even bigger neural networks!\n",
        "\n",
        "But what if our neural network is really big? How do we handle training it across multiple devices?\n",
        "\n",
        "I'm glad you asked."
      ],
      "metadata": {
        "id": "wGp3TpOQOxzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelism in JAX 💻💻\n",
        "Parallelism in JAX is handled by another fundamental transform function: pmap\n",
        "\n",
        "`pmap` basics"
      ],
      "metadata": {
        "id": "qQ0eTTKYP5eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "jax.devices()  # Output in my case for one device:( -> [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]"
      ],
      "metadata": {
        "id": "hUsnty6QO_Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(5)  # signal\n",
        "w = np.array([2., 3., 4.])  # window/kernel\n",
        "\n",
        "def convolve(w, x):  # implementation of 1D convolution/correlation\n",
        "    output = []\n",
        "\n",
        "    for i in range(1, len(x) - 1):\n",
        "        output.append(jnp.dot(x[i - 1:i + 2], w))\n",
        "\n",
        "    return jnp.array(output)\n",
        "\n",
        "result = convolve(w, x)\n",
        "print(repr(result))"
      ],
      "metadata": {
        "id": "Yxgj1HCwQAqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = jax.local_device_count()\n",
        "print(f'Number of available devices: {n_devices}')\n",
        "\n",
        "# Let's now imagine we have a much heavier load (a batch of examples)\n",
        "xs = np.arange(5 * n_devices).reshape(-1, 5)\n",
        "ws = np.stack([w] * n_devices)\n",
        "\n",
        "print(xs.shape, ws.shape)"
      ],
      "metadata": {
        "id": "J9JIvo0zRDzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First way to optimize this is to simply use vmap\n",
        "vmap_result = jax.vmap(convolve)(ws, xs)\n",
        "print(repr(vmap_result))"
      ],
      "metadata": {
        "id": "1cajgc-BRIi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The amazing thing is if you just swap vmap for pmap you are now running on multiple\n",
        "# devices. How cool is that?\n",
        "pmap_result = jax.pmap(convolve)(ws, xs)\n",
        "print(repr(pmap_result))  # ShardedDeviceArray!"
      ],
      "metadata": {
        "id": "Ws1eESl6RN__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No cross-device communication costs. Computations are done independently on each dev.\n",
        "double_pmap_result = jax.pmap(convolve)(jax.pmap(convolve)(ws, xs), xs)\n",
        "print(repr(double_pmap_result))"
      ],
      "metadata": {
        "id": "9LU_vLGDRQrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same results but we don't have to manually broadcast w (recall: same as for vmap!)\n",
        "pmap_smarter_result = jax.pmap(convolve, in_axes=(None, 0))(w, xs)\n",
        "print(repr(pmap_smarter_result))"
      ],
      "metadata": {
        "id": "QL6RmG44RjR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is all great, but oftentimes we do need to communicate between devices. 📱\n",
        "\n",
        "Let's see how that is handled."
      ],
      "metadata": {
        "id": "lhVjMdrDTdh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Communication between devices 📱📱📱📱"
      ],
      "metadata": {
        "id": "iDc5_og4TfE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Same example as above but this time we communicate across devices\n",
        "# in order to normalize the outputs\n",
        "\n",
        "def normalized_convolution(w, x):\n",
        "    output = []\n",
        "\n",
        "    for i in range(1, len(x) - 1):\n",
        "        output.append(jnp.dot(x[i - 1:i + 2], w))\n",
        "\n",
        "    output = jnp.array(output)  # same result as before\n",
        "\n",
        "    return output / jax.lax.psum(output, axis_name=\"batch_dim\")  # this is where communication happens\n",
        "\n",
        "res_pmap = jax.pmap(normalized_convolution, axis_name=\"batch_dim\", in_axes=(None, 0))(w, xs)\n",
        "res_vmap = jax.vmap(normalized_convolution, axis_name=\"batch_dim\", in_axes=(None, 0))(w, xs)\n",
        "\n",
        "print(repr(res_pmap))\n",
        "print(repr(res_vmap))\n",
        "\n",
        "print(f'Verify the output is normalized: {sum(res_pmap[:, 0])}') # your output is might be different due to num of devices"
      ],
      "metadata": {
        "id": "nAizBkEISDQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great - before we train yet another ML model (this time on multiple devices) let me show you a couple more useful functions!"
      ],
      "metadata": {
        "id": "JvkkJTa_U26N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes aside from grads we also need to return the loss value (for logging, etc.)\n",
        "\n",
        "def sum_squared_error(x, y):\n",
        "    return sum((x - y)**2)\n",
        "\n",
        "\n",
        "x = jnp.arange(4, dtype=jnp.float32)\n",
        "y = x + 0.1\n",
        "\n",
        "# An efficient way to return both grads and loss value\n",
        "jax.value_and_grad(sum_squared_error)(x, y)"
      ],
      "metadata": {
        "id": "hp9kyZ5kUmyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And sometimes the loss function needs to return intermediate results\n",
        "\n",
        "def sum_squared_error_with_aux(x, y):\n",
        "    return sum((x - y)**2), x - y\n",
        "\n",
        "\n",
        "jax.grad(sum_squared_error_with_aux, has_aux=True)(x, y)  # has_aux=True"
      ],
      "metadata": {
        "id": "kP3BGEy5VHVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How has_aux=True works:\n",
        "When has_aux=True is provided to jax.grad:\n",
        "\n",
        "1. JAX executes the function f(x, y).\n",
        "2. It uses the first element of the output tuple (the loss value) to compute the gradient.\n",
        "3. It discards the auxiliary data during the backpropagation step (as it's not used in the gradient computation).\n",
        "4. It returns a tuple: (gradient, auxiliary_data)."
      ],
      "metadata": {
        "id": "j1B4lWWIX6Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient, aux_diff = jax.grad(sum_squared_error_with_aux, has_aux=True)(x, y)\n",
        "\n",
        "print(\"Auxiliary Differences:\", aux_diff)\n",
        "print(\"Gradient:\", gradient)"
      ],
      "metadata": {
        "id": "8k2IaDalVc2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Combining value_and_grad with Auxiliary Output"
      ],
      "metadata": {
        "id": "Hdz9plqyYKJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: value_and_grad returns (value, grad)\n",
        "\n",
        "value_and_grad_fn = jax.value_and_grad(sum_squared_error_with_aux, has_aux=True)\n",
        "\n",
        "# The output format is: ((loss_value, aux_data), gradient)\n",
        "(loss, aux_diff), gradient = value_and_grad_fn(x, y)\n",
        "\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Auxiliary Differences:\", aux_diff)\n",
        "print(\"Gradient:\", gradient)"
      ],
      "metadata": {
        "id": "5euBiJgPWDWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now ready to see the most complex model pipeline so far in JAX."
      ],
      "metadata": {
        "id": "CbQRgrSncCxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a very simple model in parallel!"
      ],
      "metadata": {
        "id": "Qtuit_GAcDVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Params(NamedTuple):\n",
        "    weight: jnp.array\n",
        "    bias: jnp.array\n",
        "\n",
        "lr = 0.005\n",
        "\n",
        "def init_model(rng):\n",
        "    weights_key, bias_key = jax.random.split(rng)\n",
        "    weight = jax.random.normal(weights_key, ())\n",
        "    bias = jax.random.normal(bias_key, ())\n",
        "    return Params(weight, bias)\n",
        "\n",
        "def forward(params, xs):\n",
        "    return params.weight * xs + params.bias\n",
        "\n",
        "def loss_fn(params, xs, ys):\n",
        "    pred = forward(params, xs)\n",
        "    return jnp.mean((pred - ys)**2)  # MSE\n",
        "\n",
        "@functools.partial(jax.vmap, axis_name=\"batch\")\n",
        "def update(params, xs, ys):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, xs, ys)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name=\"batch\")\n",
        "\n",
        "    # Each device performs its own SGD update, but since we start with the same params\n",
        "    # and synchronise gradients, the params stay in sync on each device\n",
        "    new_params = jax.tree.map(\n",
        "        lambda param, g: param - g * lr, params, grads\n",
        "    )\n",
        "\n",
        "    # If we were using Adam or another stateful optimizer,\n",
        "    # we would also do something like:\n",
        "    # updates, new_optimizer_state = optimizer(grads, optimizer_state)\n",
        "    # and then use updates instead of grads to actually update the params.\n",
        "    # (And we'd include the new_optimizer_state in the output, naturally.)\n",
        "\n",
        "    return new_params, loss"
      ],
      "metadata": {
        "id": "RIWTalM6YTzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_w, true_b = 2, -1\n",
        "xs = np.random.normal(size=(128, 1))\n",
        "noise = 0.5 * np.random.normal(size=(128, 1))\n",
        "ys = xs * true_w + true_b + noise\n",
        "\n",
        "plt.scatter(xs, ys)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "npaoGvf-ekH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = 0\n",
        "params = init_model(jax.random.PRNGKey(state))\n",
        "n_devices = jax.local_device_count()\n",
        "replicated_params = jax.tree.map(lambda x: jnp.array([x] * n_devices), params)\n",
        "print(replicated_params)\n",
        "\n",
        "def reshape_for_pmap(data, n_device):\n",
        "    return data.reshape(n_devices, data.shape[0] // n_devices, *data.shape[1:])\n",
        "\n",
        "x_parallel = reshape_for_pmap(xs, n_devices)\n",
        "y_parallel = reshape_for_pmap(ys, n_devices)\n",
        "\n",
        "print(x_parallel.shape, y_parallel.shape)"
      ],
      "metadata": {
        "id": "db3D9m7TezOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def type_after_update(name, obj):\n",
        "    print(f\"After the first 'update()', '{name}' is a {type(obj)}\")\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # This is where the params and data gets communicated to devices\n",
        "    replicated_params, loss = update(replicated_params, x_parallel, y_parallel)\n",
        "\n",
        "    # replicated_params and loss are now both ShardedDeviceArrays,\n",
        "    # indicating that they're on the devices.\n",
        "    # x/y_parallel remains a NumPy array on the host (simulating data streaming).\n",
        "    if epoch == 0:\n",
        "        type_after_update('replicated_params.weight', replicated_params.weight)\n",
        "        type_after_update('loss', loss)\n",
        "        type_after_update('x_parallel', x_parallel)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(loss.shape)\n",
        "        print(f\"Step {epoch:3d}, loss: {loss[0]:.3f}\")\n",
        "\n",
        "# Like the loss, the leaves of params have an extra leading dimension,\n",
        "# so we take the params from the first device.\n",
        "params = jax.device_get(jax.tree.map(lambda x: x[0], replicated_params))"
      ],
      "metadata": {
        "id": "OGC7uuhfnwfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(xs, ys, c='g', marker='.', label='GT')\n",
        "plt.plot(xs, forward(params, xs), c='r', label=\"Prediction\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SPKN-LCuqp6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wohoo! We trained a model in parallel on multiple devices (in my case one, but should work for multiple devices)! 🥳🥳🥳\n",
        "\n",
        "There are more super important tools we're missing though.\n",
        "\n",
        "1. How would we go about transfer learning in JAX?\n",
        "2. How do we freeze certain layers and fine tune others?\n",
        "3. How do we get per sample gradients (and not the usual per batch gradients)?\n",
        "And so on. That brings us to the last section!"
      ],
      "metadata": {
        "id": "NtpUqwwwsbed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Autodiff 🚀\n",
        "\n",
        "### Stop gradients\n",
        "`jax.lax.stop_gradient` is the primitive used for this purpose.\n",
        "**Example 1: TD(0) update (RL algorithm).**\n",
        "The TD(0) update to the network parameters is:\n",
        "$$\\Delta \\theta = (r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})) \\nabla v_{\\theta}(s_{t-1})\n",
        "$$\n",
        "This update is not the gradient of any loss function.\n",
        "However, it can be **written** as the gradient of the pseudo loss function\n",
        "\n",
        "$$L(\\theta) = [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})]^2$$\n",
        "if the dependency of the target $r_t + v_{\\theta}(s_t)$ on the parameter $\\theta$ is ignored.\""
      ],
      "metadata": {
        "id": "RlTj1Q0Iso46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# value function (simple linear fn) and initial parameters\n",
        "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
        "theta = jnp.array([0.1, -0.1, 0.])\n",
        "\n",
        "# an example transition\n",
        "s_tm1 = jnp.array([1., 2., -1.])\n",
        "r_t = jnp.array(1.)\n",
        "s_t = jnp.array([2., 1., 0.])\n",
        "\n",
        "def td_loss(theta, s_tm1, r_t, s_t):\n",
        "    v_tm1 = value_fn(theta, s_tm1)\n",
        "    target = r_t + value_fn(theta, s_t)\n",
        "    return (jax.lax.stop_gradient(target) - v_tm1)**2\n",
        "\n",
        "td_update = jax.grad(td_loss)\n",
        "delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
        "\n",
        "delta_theta"
      ],
      "metadata": {
        "id": "45UtSWyMsV0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2: straight-through estimator**\n",
        "\n",
        "(used in e.g. VQ-VAE, check out my video: https://www.youtube.com/watch?v=VZFVUrYcig0)"
      ],
      "metadata": {
        "id": "7y7fw0B2u4xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return jnp.round(x)  # non-differentiable\n",
        "\n",
        "def straight_through_f(x):\n",
        "    return x + jax.lax.stop_gradient((f(x) - x))  # f(x) = x => dfdx = 1\n",
        "\n",
        "x = 5.6\n",
        "print(f\"f(x) = {f(x)}\")\n",
        "print(\"straight_through_f(x):\", straight_through_f(x))  # same values in the forward pass\n",
        "\n",
        "print(\"grad(f)(x):\", jax.grad(f)(x))  # non-diff so it just returns 0\n",
        "print(\"grad(straight_through_f)(x):\", jax.grad(straight_through_f)(x))"
      ],
      "metadata": {
        "id": "A4eKeNeJuuND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per sample gradients\n",
        "\n",
        "\"In many frameworks (PyTorch, TF, Theano) it is often not trivial to compute per-example gradients, because the library directly accumulates the gradient over the batch. Naive workarounds, such as computing a separate loss per example and then aggregating the resulting gradients are typically very inefficient.\""
      ],
      "metadata": {
        "id": "7o40SUEOxDJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is how to do it in JAX!\n",
        "\n",
        "# Batch the data\n",
        "\n",
        "batched_s_tm1 = jnp.stack([s_tm1, s_tm1])\n",
        "batched_r_t = jnp.stack([r_t, r_t])\n",
        "batched_s_t = jnp.stack([s_t, s_t])\n",
        "\n",
        "perex_grads = jax.jit(jax.vmap(jax.grad(td_loss), in_axes=(None, 0, 0, 0)))\n",
        "perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)"
      ],
      "metadata": {
        "id": "AfEAsFUYvHKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced stuff 🐉\n",
        "JAX autodiff engine is very powerful.\n",
        "\n",
        "You can do various things like define custom derivative rules, etc. Take a look at advanced tutorials in the docs (e.g. [JAX: Autodiff Cookbook](https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html))\n",
        "\n",
        "Here is an example of how to calculate update for MAML (notice how similar the code is to math!)\n",
        "\n",
        "![](https://github.com/Xrenya/quickstart_JAX/blob/main/images/meta-learning.png?raw=true)\n",
        "\n",
        "![](https://github.com/Xrenya/quickstart_JAX/blob/main/images/meta-curve.png?raw=true)\n",
        "\n",
        "```python\n",
        "# Very easy to do in JAX\n",
        "def meta_loss_fn(params, data):\n",
        "    \"\"\"Computes the loss after one step of SGD.\"\"\"\n",
        "    grads = jax.grad(loss_fn)(params, data)\n",
        "    return loss_fn(params - lr * grads, data)\n",
        "\n",
        "meta_grads = jax.grad(meta_loss_fn)(params, data)\n",
        "```"
      ],
      "metadata": {
        "id": "gr43f-ny0g9I"
      }
    }
  ]
}