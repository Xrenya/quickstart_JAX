{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWuFoZlJ7hKs"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "from jax import lax  # JAX's low level API, just anagram for XLA\n",
        "\n",
        "from jax import make_jaxpr\n",
        "from jax import random\n",
        "from jax import device_put\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_np = np.linspace(0, 10, 1000)\n",
        "y_np = 2 * np.sin(x_np) * np.cos(x_np)\n",
        "plt.plot(x_np, y_np)"
      ],
      "metadata": {
        "id": "vcEoiJ3y79Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_jnp = jnp.linspace(0, 10, 1000)\n",
        "y_jnp = 2 * jnp.sin(x_jnp) * jnp.cos(x_jnp)\n",
        "plt.plot(x_jnp, y_jnp)"
      ],
      "metadata": {
        "id": "ve1sMuvp-g6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TypeError: JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method:\n",
        "size = 10\n",
        "index = 0\n",
        "value = 23\n",
        "\n",
        "x = np.arange(size)\n",
        "print(x)\n",
        "x[index] = value\n",
        "print(x)"
      ],
      "metadata": {
        "id": "fuMaR0NqzHWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 10\n",
        "index = 0\n",
        "value = 23\n",
        "\n",
        "x = jnp.arange(size)  # immutable arrays\n",
        "print(x)\n",
        "x[index] = value\n",
        "print(x)"
      ],
      "metadata": {
        "id": "PRAFnR6A1v9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.at[index].set(value)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "DznXbpkE1z9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "key = random.PRNGKey(seed) # Create a legacy PRNG key given an integer seed.\n",
        "\n",
        "x = random.normal(key, (10,))\n",
        "print(type(x), x)"
      ],
      "metadata": {
        "id": "J1f_HGai19Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "key = random.key(seed) # It is recommended for use instead.\n",
        "\n",
        "x = random.normal(key, (10,))\n",
        "print(type(x), x)"
      ],
      "metadata": {
        "id": "BQewqfFY2slO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 3000\n",
        "\n",
        "# Data is automagically pushed to the AI accelerator! (DeviceArray structure)\n",
        "# No more need for \".to(device)\" (PyTorch syntax)\n",
        "x_jnp = random.normal(key, (size, size), dtype=jnp.float32)\n",
        "x_np = np.random.normal(size=(size, size)).astype(np.float32)\n",
        "\n",
        "%timeit jnp.dot(x_jnp, x_jnp.T).block_until_ready() # GPU or TPU\n",
        "%timeit np.dot(x_np, x_np.T) # CPU\n",
        "%timeit jnp.dot(x_np, x_np.T).block_until_ready() # GPU or TPU with transfer overhead\n",
        "\n",
        "x_np_device = device_put(x_np) # numpu to GPU\n",
        "%timeit jnp.dot(x_np_device, x_np_device.T).block_until_ready() # GPU\n",
        "\n",
        "# block_until_ready() -> asynchronous dispatch"
      ],
      "metadata": {
        "id": "JTzMplWN2xjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# jit()\n",
        "\n",
        "jit compiles your functions using XLA and caches them -> speeeeed ðŸš€"
      ],
      "metadata": {
        "id": "L6mI65Vq41nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_fn(fn, l=-10, r=10, n=1000):\n",
        "    x = np.linspace(l, r, num=n)\n",
        "    y = fn(x)\n",
        "    plt.plot(x, y); plt.show()"
      ],
      "metadata": {
        "id": "oDohSIIK4gsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "\n",
        "\n",
        "silu_jit = jit(selu)\n",
        "\n",
        "visualize_fn(silu_jit)\n",
        "\n",
        "data = random.normal(key, (1_000_000,))\n",
        "\n",
        "print(\"non-jit version:\")\n",
        "%timeit selu(data).block_until_ready()\n",
        "\n",
        "print(\"jit version:\")\n",
        "%timeit silu_jit(data).block_until_ready()"
      ],
      "metadata": {
        "id": "UdfbgqhS5GiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# grad()\n",
        "\n",
        "Differentation can be:\n",
        "\n",
        "- manual\n",
        "- symbolic\n",
        "- numeric\n",
        "- automatic! â¤ï¸"
      ],
      "metadata": {
        "id": "nOH9NdcA6F8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# automatic!\n",
        "\n",
        "def sum_log(x):\n",
        "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
        "\n",
        "x = jnp.arange(3.)\n",
        "loss = sum_log\n",
        "\n",
        "# By default grad calculates the derivative of a fn w.r.t. 1st parameter!\n",
        "# Here we bundled inputs into a 1st param so it doesn't matter.\n",
        "grad_loss = grad(loss)\n",
        "print(x)\n",
        "print(grad_loss(x))"
      ],
      "metadata": {
        "id": "V1-HLXHj5vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric diff (to double check that autodiff works correctly)\n",
        "# A finite difference is a mathematical expression of the form f(x + b) âˆ’ f(x + a). Finite differences are often used as approximations of derivatives\n",
        "def finite_differences(f, x):\n",
        "    eps = 1e-3\n",
        "    return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps) for v in jnp.eye(len(x))])\n",
        "\n",
        "print(finite_differences(loss, x))"
      ],
      "metadata": {
        "id": "z88Vl5NQB2nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 1.\n",
        "\n",
        "f = lambda x: x**2 + x + 4\n",
        "visualize_fn(f, l=-1, r=2, n=100)\n",
        "\n",
        "dfdx = grad(f) # 2*x + 1\n",
        "d2fdx2 = grad(dfdx) # 2\n",
        "d3fdx3 = grad(d2fdx2) # 0\n",
        "\n",
        "print(f\"f(x) = {f(x)} -> \", f\"dfdx(x) = {dfdx(x)} -> \", f\"d2fdx2(x) = {d2fdx2(x)} ->\", f\"d3fdx3(x) = {d3fdx3(x)}\")"
      ],
      "metadata": {
        "id": "mhIFtDKfCONg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX autodiff engine is very powerful (\"advanced\" example)\n",
        "\n",
        "from jax import jacfwd, jacrev\n",
        "\n",
        "f = lambda x, y: x**2 + y**2\n",
        "\n",
        "# df/dx = 2x\n",
        "# df/dy = 2y\n",
        "# J = [df/dx, df/dy]\n",
        "\n",
        "# d2f/dx2 = 2\n",
        "# d2f/dy2 = 2\n",
        "# d2f/dxdy = 0\n",
        "# d2f/dydx = 0\n",
        "# H = [[d2f/dx, d2f/dxdy], [d2f/dydx, d2f/dy]]\n",
        "\n",
        "def hessian(f):\n",
        "    return jit(jacfwd(jacrev(f, argnums=(0, 1)), argnums=(0, 1)))\n",
        "\n",
        "print(f\"Jacobian = {jacrev(f, argnums=(0, 1))(1., 1.)}\")\n",
        "print(f\"Full Hessian = {hessian(f)(1., 1.)}\")"
      ],
      "metadata": {
        "id": "PV_c0gPJFQ6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge case |x|, how does JAX handle it?\n",
        "\n",
        "f = lambda x: abs(x)\n",
        "visualize_fn(f)\n",
        "\n",
        "print(f\"f(-1) = {f(-1)}, f(1) = {f(1)}\")\n",
        "dfdx = grad(f)\n",
        "print(f\"dfdx(0.) = {dfdx(0.)}\")\n",
        "print(f\"dfdx(0.001) = {dfdx(0.001)}\")"
      ],
      "metadata": {
        "id": "cErOazKVG7a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vmap() 101\n",
        "\n",
        "Write your functions as if you were dealing with a single datapoint!"
      ],
      "metadata": {
        "id": "Vfgu_hlJRGzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = random.normal(key, (150, 100))\n",
        "batch_x = random.normal(key, (10, 100))\n",
        "\n",
        "def apply_matrix(x):\n",
        "    return jnp.dot(x, W.T) # (10, 100) @ (150, 100) -> (150, 10)\n",
        "\n",
        "apply_matrix(batch_x)"
      ],
      "metadata": {
        "id": "-CabdaerIq6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naively_batched_apply_matrix(batched_x):\n",
        "    return jnp.stack([apply_matrix(x) for x in batched_x])\n",
        "\n",
        "print(\"Naively batched\")\n",
        "%timeit naively_batched_apply_matrix(batch_x).block_until_ready()"
      ],
      "metadata": {
        "id": "C9PYw8dqRj-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def batched_apply_matrix(batched_x):\n",
        "    return jnp.dot(x, W.T)\n",
        "\n",
        "print(\"Manually batched\")\n",
        "%timeit batched_apply_matrix(batch_x).block_until_ready()"
      ],
      "metadata": {
        "id": "HcAjEzZ9RsaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_matrix(x):\n",
        "    return jnp.dot(W, x)\n",
        "\n",
        "@jit  # Note: we can arbitrarily compose JAX transforms! Here jit + vmap.\n",
        "def vmap_batched_apply_matrix(batched_x):\n",
        "    return vmap(apply_matrix)(batched_x)\n",
        "\n",
        "print(\"Auto-vectorized\")\n",
        "%timeit vmap_batched_apply_matrix(batch_x).block_until_ready()"
      ],
      "metadata": {
        "id": "_o3E2ta-S0SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vmap_batched_apply_matrix(batch_x).block_until_ready().shape"
      ],
      "metadata": {
        "id": "OkU5dH3MT_fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_matrix(x):\n",
        "    return jnp.dot(W, x)\n",
        "\n",
        "@jit  # Note: we can arbitrarily compose JAX transforms! Here jit + vmap.\n",
        "def vmap_batched_apply_matrix(batched_x):\n",
        "    return vmap(apply_matrix, in_axes=(0), out_axes=(0))(batched_x)\n",
        "\n",
        "%timeit vmap_batched_apply_matrix(batch_x).block_until_ready()"
      ],
      "metadata": {
        "id": "e1Jw992dUL0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vmap_batched_apply_matrix(batch_x).block_until_ready().shape"
      ],
      "metadata": {
        "id": "y3QSXWYFUkh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_matrix(x):\n",
        "    return jnp.dot(W, x)\n",
        "\n",
        "@jit  # Note: we can arbitrarily compose JAX transforms! Here jit + vmap.\n",
        "def vmap_batched_apply_matrix(batched_x):\n",
        "    return vmap(apply_matrix)(batched_x)\n",
        "\n",
        "vmap_batched_apply_matrix(batch_x).block_until_ready().shape"
      ],
      "metadata": {
        "id": "9sp_zrC4XpPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.expand_dims(batch_x, 0).shape"
      ],
      "metadata": {
        "id": "KiVlrZzgY45u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_matrix(x):\n",
        "    return jnp.dot(W, x)\n",
        "\n",
        "@jit  # Note: we can arbitrarily compose JAX transforms! Here jit + vmap.\n",
        "def vmap_batched_apply_matrix(batched_x):\n",
        "    return vmap(vmap(apply_matrix, in_axes=0), in_axes=0)(batched_x)\n",
        "\n",
        "vmap_batched_apply_matrix(jnp.expand_dims(batch_x, 0)).block_until_ready().shape"
      ],
      "metadata": {
        "id": "T-InRQfbZHp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: lax is stricter\n",
        "\n",
        "print(jnp.add(1, 1.0))  # jax.numpy API implicitly promotes mixed types\n",
        "print(lax.add(1, 1.0))  # jax.lax API requires explicit type promotion"
      ],
      "metadata": {
        "id": "eBBdNq1nZQ2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: lax is more powerful (but as a tradeoff less user-friendly)\n",
        "\n",
        "x = jnp.array([1, 2, 1])\n",
        "y = jnp.ones(10)\n",
        "\n",
        "result1 = jnp.convolve(x, y)\n",
        "\n",
        "result2 = lax.conv_general_dilated(\n",
        "    x.reshape(1, 1, 3).astype(float), # explicit float\n",
        "    y.reshape(1, 1, 10),\n",
        "    window_strides=(1,),\n",
        "    padding=[(len(y) - 1, len(y) - 1)] # padding='full' numpy\n",
        ")\n",
        "\n",
        "print(result1)\n",
        "print(result2)\n"
      ],
      "metadata": {
        "id": "04fz-VinfUDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does JIT actually work?"
      ],
      "metadata": {
        "id": "hjZPBhsk1USL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x):\n",
        "    x -= x.mean(0)\n",
        "    return x / x.std(0)\n",
        "\n",
        "norm_compiled = jit(norm)\n",
        "\n",
        "x = random.normal(key, (10_000, 100), dtype=jnp.float32)\n",
        "\n",
        "%timeit norm(x).block_until_ready()\n",
        "%timeit norm_compiled(x).block_until_ready()"
      ],
      "metadata": {
        "id": "-qtqBjTbf7Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative(x):\n",
        "    return x[x < 0]\n",
        "\n",
        "x = random.normal(key, (10,), dtype=jnp.float32)\n",
        "print(get_negative(x))"
      ],
      "metadata": {
        "id": "msROABgdEwdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jit(get_negative)(x))"
      ],
      "metadata": {
        "id": "M98yOeINFvS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This error occurs when a program attempts to use non-concrete boolean indices in a traced indexing operation. Under JIT compilation, JAX arrays must have static shapes (i.e. shapes that are known at compile-time) and so boolean masks must be used carefully. Some logic implemented via boolean masking is simply not possible in a jax.jit() function; in other cases, the logic can be re-expressed in a JIT-compatible way, often using the three-argument version of where()."
      ],
      "metadata": {
        "id": "Q6sVh1VqGisw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative(x):\n",
        "    return jnp.where(x > 0, x, 0)\n",
        "\n",
        "x = random.normal(key, (10,), dtype=jnp.float32)\n",
        "print(jit(get_negative)(x))"
      ],
      "metadata": {
        "id": "OmSUeGHlFzYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def f(x, y):\n",
        "    print(\"Running f():\")\n",
        "    print(f\"x = {x}\")\n",
        "    print(f\"y = {y}\")\n",
        "    result = jnp.dot(x + 1, y + 1)\n",
        "    print(f\"result = {result}\")\n",
        "    return result\n",
        "\n",
        "x = np.random.randn(3, 4)\n",
        "y = np.random.randn(4)\n",
        "print(f(x, y))\n",
        "\n",
        "x2 = np.random.randn(3, 4)\n",
        "y2 = np.random.randn(4)\n",
        "print(f(x2, y2)) # Oops! Side effects (like print) are not compiled...\n",
        "\n",
        "# Note: any time we get the same shapes and types we just call the compiled fn!"
      ],
      "metadata": {
        "id": "SKsXOdbiGLH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is by design. JAXâ€™s goal is to compile pure functions â€” functions without side effects (like printing, file I/O, modifying global variables).\n",
        "\n",
        "If you put a print, logging, or any other side effect inside a @jit function, it will only run during the first trace â€” not on subsequent calls.\n",
        "\n",
        "**Reusing the compiled graph makes subsequent calls extremely fast â€” often 10xâ€“100x faster than raw NumPy.**"
      ],
      "metadata": {
        "id": "gnZx74ByTSyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, y):\n",
        "    return jnp.dot(x + 1, y + 1)\n",
        "\n",
        "print(make_jaxpr(f)(x, y))"
      ],
      "metadata": {
        "id": "jf_oCGCTSYTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def f(x, neg):\n",
        "    return -x if neg else x\n",
        "\n",
        "f(1, True)"
      ],
      "metadata": {
        "id": "mZrqCc-ZXIiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def f(x, neg):\n",
        "    print(x)\n",
        "    return -x if neg else x\n",
        "\n",
        "print(f(1, True))\n",
        "print(f(2, True))\n",
        "print(f(2, False))\n",
        "print(f(23, False))"
      ],
      "metadata": {
        "id": "R8puChJPXTWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def f(x):\n",
        "    print(\"expand dim\")\n",
        "    return x.reshape(jnp.array(x.shape).prod())\n",
        "\n",
        "\n",
        "x = jnp.ones((2, 3))\n",
        "f(x)"
      ],
      "metadata": {
        "id": "VcgCNYbkYndO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸš« During tracing, JAX does NOT allow converting abstract shapes into concrete arrays via jnp.array.  \n",
        "Why? Because jnp.array([2,3]) creates a concrete array, but during tracing, weâ€™re still building a symbolic graph â€” we donâ€™t have real values yet. JAX wants to keep everything symbolic until runtime."
      ],
      "metadata": {
        "id": "0sSt6JwRbTq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Workaround: using numpy instead of jax.numpy\n",
        "\n",
        "\n",
        "@jit\n",
        "def f(x):\n",
        "    return x.reshape(np.array(x.shape).prod())\n",
        "\n",
        "\n",
        "x = jnp.ones((2, 3))\n",
        "f(x)"
      ],
      "metadata": {
        "id": "BQgJ5wEVauAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pure functions\n",
        "JAX is designed to work only on pure functions!\n",
        "Pure function? Informal definition:\n",
        "\n",
        "1. All the input data is passed through the function parameters, all the results are output through the function results.\n",
        "2. A pure function will always return the same result if invoked with the same inputs."
      ],
      "metadata": {
        "id": "eNTMT42Cf6ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def impure_print_side_effect(x):\n",
        "    print(\"Execution function\")\n",
        "    return x\n",
        "\n",
        "\n",
        "print(\"First call: \", jit(impure_print_side_effect)(4.))\n",
        "\n",
        "\n",
        "print(\"Second call: \", jit(impure_print_side_effect)(5.))\n",
        "\n",
        "\n",
        "print(\"Third call: \", jit(impure_print_side_effect)(jnp.array([1.])))"
      ],
      "metadata": {
        "id": "28MI335xa1ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = 0.\n",
        "\n",
        "def impure_use_global(x):\n",
        "    print(\"Execution function\")\n",
        "    return x + g\n",
        "\n",
        "\n",
        "print(\"First call: \", jit(impure_use_global)(4.))\n",
        "\n",
        "g = 10.\n",
        "\n",
        "# Subsequent runs may silently use the cached value of the globals\n",
        "print(\"Second call: \", jit(impure_use_global)(5.))\n",
        "\n",
        "# This will end up reading the latest value of the global after recompile\n",
        "print(\"Thrid call: \", jit(impure_use_global)(jnp.array([4.])))"
      ],
      "metadata": {
        "id": "FX0MsuVugodZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pure_use_internal_state(x):\n",
        "    state = dict(even=0, odd=0)\n",
        "    for i in range(10):\n",
        "        state[\"even\" if i % 2 == 0 else \"odd\"] += x\n",
        "    return state[\"even\"] + state[\"odd\"]\n",
        "\n",
        "print(jit(pure_use_internal_state)(5.))"
      ],
      "metadata": {
        "id": "BolUzRjcht3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array = jnp.arange(10)\n",
        "print(lax.fori_loop(0, 10, lambda i, x: x + array[i], 0))  # expected result 45\n",
        "\n",
        "iterator = iter(range(10))\n",
        "print(lax.fori_loop(0, 10, lambda i, x: x + next(iterator), 0))  # unexpected result 0\n",
        "\n",
        "\"\"\" The semantics of fori_loop are given by this Python implementation:\n",
        "def fori_loop(lower, upper, body_fun, init_val):\n",
        "  val = init_val\n",
        "  for i in range(lower, upper):\n",
        "    val = body_fun(i, val)\n",
        "  return val\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YLEDmgWKiVDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In-Place Updates"
      ],
      "metadata": {
        "id": "XBGXkTT4qdcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax_array = jnp.zeros((3, 3), dtype=jnp.float32)\n",
        "updated_array = jax_array.at[1, :].set(1.0)\n",
        "\n",
        "print(f\"Original: {jax_array}\")\n",
        "print(f\"Updated: {updated_array}\")"
      ],
      "metadata": {
        "id": "JcUn3SKXmvJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Origibal array:\")\n",
        "jax_array = jnp.ones((5, 6))\n",
        "print(jax_array)\n",
        "\n",
        "print(\"Updated array:\")\n",
        "new_jax_array = jax_array.at[::2, 3:].add(7.)\n",
        "print(new_jax_array)"
      ],
      "metadata": {
        "id": "dXG8iu1wqw_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Out-of-Bounds Indexing"
      ],
      "metadata": {
        "id": "J-hDV6qduAur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  np.arange(10)[11]\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ],
      "metadata": {
        "id": "sktiGqg3rcYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX behavior\n",
        "# 1) updates at out-of-bounds indices are skipped\n",
        "# 2) retrievals result in index being clamped\n",
        "# in general there are currently some bugs so just consider the behavior undefined!\n",
        "\n",
        "print(jnp.arange(10).at[11].add(23))  # example of 1)\n",
        "print(jnp.arange(10)[11])  # example of 2)"
      ],
      "metadata": {
        "id": "JtOq8wemuC7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-array inputs\n",
        "This is added by design (performance reasons)"
      ],
      "metadata": {
        "id": "xKWUuY3ZuQ1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum([1, 2, 3]))"
      ],
      "metadata": {
        "id": "oRpyGW-HuGLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    jnp.sum([1, 2, 3])\n",
        "except TypeError as e:\n",
        "    print(f\"TypeError: {e}\")"
      ],
      "metadata": {
        "id": "gpddICdmuUh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(jnp.sum(jnp.array([1, 2, 3])))\n",
        "except TypeError as e:\n",
        "    print(f\"TypeError: {e}\")"
      ],
      "metadata": {
        "id": "vfwCsY3suej2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def permissive_sum(x):\n",
        "    return jnp.sum(jnp.array(x))\n",
        "\n",
        "x = list(np.arange(10))\n",
        "print(make_jaxpr(permissive_sum)(x))"
      ],
      "metadata": {
        "id": "2tL7KdgzvWiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### random numbers"
      ],
      "metadata": {
        "id": "ml7A7mewxYPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.random.random())\n",
        "print(np.random.random())\n",
        "\n",
        "np.random.seed(seed)\n",
        "\n",
        "rng_state = np.random.get_state()\n",
        "print(rng_state[2:])\n",
        "\n",
        "_ = np.random.uniform()\n",
        "rng_state = np.random.get_state()\n",
        "print(rng_state[2:])\n",
        "\n",
        "\n",
        "_ = np.random.uniform()\n",
        "rng_state = np.random.get_state()\n",
        "print(rng_state[2:])\n",
        "# Mersenne Twister PRNG is known to have a number of problems (NumPy's imp of PRNG)"
      ],
      "metadata": {
        "id": "-p6Ana1hvoYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = random.PRNGKey(seed)\n",
        "print(key)\n",
        "\n",
        "print(random.normal(key, shape=(1,)))\n",
        "print(key)\n",
        "\n",
        "print(random.normal(key, shape=(1,))) # same result!!\n",
        "print(key)\n"
      ],
      "metadata": {
        "id": "gt6tPMlAxfxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution? -> Split every time you need a pseudorandom number.\n",
        "\n",
        "print(f\"Key = {key}\")\n",
        "key, subkey = random.split(key)\n",
        "normal_pseudorandom = random.normal(subkey, shape=(1,))\n",
        "print(\"    \\---SPLIT --> new key   \", key)\n",
        "print(\"             \\--> new subkey\", subkey, \"--> normal\", normal_pseudorandom)\n",
        "# Note1: you can also split into more subkeys and not just 1\n",
        "# Note2: key, subkey no difference it's only a convention"
      ],
      "metadata": {
        "id": "KIxuXZtayCsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why this design?\n",
        "# Well...think...with current design can the code be:\n",
        "# 1) reproducible?\n",
        "# 2) parallelizable?\n",
        "# 3) vectorisable?\n",
        "\n",
        "np.random.seed(seed)\n",
        "\n",
        "def bar():\n",
        "    return np.random.uniform()\n",
        "def baz():\n",
        "    return np.random.uniform()\n",
        "def foo():\n",
        "    return bar() + 2 * baz()\n",
        "\n",
        "print(foo())\n",
        "# What if we want to parallelize this code? NumPy assumes too much. 2) is violated."
      ],
      "metadata": {
        "id": "R_RHHho0ycs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Numpy:\")\n",
        "np.random.seed(seed)\n",
        "print(\"individuality: \", np.stack([np.random.uniform() for _ in range(3)]))\n",
        "\n",
        "np.random.seed(seed)\n",
        "print(\"all at once: \", np.random.uniform(size=3))\n",
        "\n",
        "# JAX\n",
        "print(\"JAX:\")\n",
        "key = random.PRNGKey(seed)\n",
        "subkeys = random.split(key, 3)\n",
        "sequences = np.stack([random.normal(subkey) for subkey in subkeys])\n",
        "print(\"individuality: \", sequences)\n",
        "\n",
        "\n",
        "key = random.PRNGKey(seed)\n",
        "print(\"all at once: \", random.uniform(key=key, shape=(3,)))"
      ],
      "metadata": {
        "id": "HthGuehHy0Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent question! You've identified a **fundamental flaw** in NumPy's random number generation that JAX was specifically designed to fix. The issue with NumPy violates the principle of **parallelizability** and **functional purity**.\n",
        "\n",
        "**Key differences**:\n",
        "1. **`individual` and `batch` are DIFFERENT** (as they should be â€” different key usage patterns).\n",
        "2. **But both are REPRODUCIBLE** â€” same `key` always gives same result.\n",
        "3. **No hidden dependencies** â€” you can call `random.normal(other_key)` anywhere without affecting these results.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Summary: Why JAX's Approach is Superior\n",
        "\n",
        "| Aspect | NumPy | JAX |\n",
        "|--------|-------|-----|\n",
        "| **Global State** | âŒ Mutable global RNG | âœ… Explicit immutable keys |\n",
        "| **Reproducibility** | âŒ Depends on call order | âœ… Same key = same result |\n",
        "| **Parallelization** | âŒ Race conditions | âœ… Safe (no shared state) |\n",
        "| **Functional Purity** | âŒ Side effects | âœ… Pure functions |\n",
        "| **Debugging** | âŒ Hard to trace state | âœ… Explicit key flow |\n",
        "\n",
        "---\n",
        "\n",
        "JAX's design:\n",
        "\n",
        "1. **Immutability**: Data structures shouldn't change\n",
        "2. **Reproducibility**: Same inputs â†’ same outputs  \n",
        "3. **Parallelizability**: No shared mutable state\n",
        "\n",
        "NumPy's global RNG state breaks **all three** of these principles, making it unsuitable for modern ML workflows that require:\n",
        "- **Distributed training** (multiple GPUs/nodes)\n",
        "- **Reproducible experiments**\n",
        "- **Functional programming** patterns\n",
        "\n",
        "JAX's explicit PRNG keys fix this fundamental design flaw, enabling **scalable, reproducible, and parallelizable** random number generation."
      ],
      "metadata": {
        "id": "Z7qZ2Rhv5UTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Control Flow"
      ],
      "metadata": {
        "id": "xy8txatf6xVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python control flow + grad() -> everything is ok\n",
        "def f(x):\n",
        "    if x < 3:\n",
        "        return 3. * x**2\n",
        "    else:\n",
        "        return -4. * x\n",
        "\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "y = [f(n) for n in x]\n",
        "plt.plot(x, y)\n",
        "plt.show()\n",
        "\n",
        "print(grad(f)(2.))\n",
        "print(grad(f)(4.))"
      ],
      "metadata": {
        "id": "JzljPqPD4u97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python contol flow + jit() -> issues\n",
        "\n",
        "# \"The tradeoff is that with higher levels of abstraction we gain a more general view\n",
        "# of the Python code (and thus save on re-compilations),\n",
        "# but we require more constraints on the Python code to complete the trace.\"\n",
        "\n",
        "# Example 1: conditioning on value (same function as in the above cell)\n",
        "# Solution (recall: we already have seen this)\n",
        "\n",
        "f_jit = jit(f, static_argnums=(0,))\n",
        "x = 2.0\n",
        "\n",
        "print(make_jaxpr(f_jit, static_argnums=(0,))(x))\n",
        "print(f_jit(x))"
      ],
      "metadata": {
        "id": "OxFe-Cjt8l8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: range depends on value again\n",
        "\n",
        "def f(x, n):\n",
        "    y = 0.\n",
        "    for i in range(n):\n",
        "        y += x[i]\n",
        "    return y\n",
        "\n",
        "f_jit = jit(f, static_argnums=(1,))\n",
        "x = (jnp.array([2., 3., 4.]), 15)\n",
        "\n",
        "print(make_jaxpr(f_jit, static_argnums=(1,))(*x))\n",
        "print(f_jit(*x))"
      ],
      "metadata": {
        "id": "Ng4j_HV9AnbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Even \"better\" (it's less readable) solution is to use low level API\n",
        "\n",
        "def f_fori(x, n):\n",
        "    body_func = lambda i, val: val + x[i]\n",
        "    return lax.fori_loop(0, n, body_func, 0.)\n",
        "\n",
        "f_fori_jit = jit(f_fori)\n",
        "\n",
        "print(make_jaxpr(f_fori_jit)(*x))\n",
        "print(f_fori_jit(*x))"
      ],
      "metadata": {
        "id": "P5XMkJbQBeOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: this is not problematic (it'll only cache a single branch)\n",
        "\n",
        "def log2_if_rank_2(x):\n",
        "    if x.ndim == 2:\n",
        "        ln_x = jnp.log(x)\n",
        "        ln_2 = jnp.log(2.0)\n",
        "        return ln_x / ln_2\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "print(make_jaxpr(log2_if_rank_2)(jnp.array([1, 2, 3])))\n",
        "print(make_jaxpr(log2_if_rank_2)(jnp.array([[1, 2, 3]])))"
      ],
      "metadata": {
        "id": "Y-4zpocxNF38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NaNs"
      ],
      "metadata": {
        "id": "GVWhST5dSfJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.divide(0., 0.)\n",
        "\n",
        "from jax import config\n",
        "\n",
        "config.update(\"jax_debug_nans\", True)"
      ],
      "metadata": {
        "id": "s3DF-wx_N82k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = random.uniform(key, (1000,), dtype=jnp.float32)\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "id": "yMN7AKrzSiu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-OfmXFJWS4W6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}